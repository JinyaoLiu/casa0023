[
  {
    "objectID": "Week1.html",
    "href": "Week1.html",
    "title": "1  Getting started with remote sensing",
    "section": "",
    "text": "2 ## References"
  },
  {
    "objectID": "Week1.html#definition",
    "href": "Week1.html#definition",
    "title": "1  1. Module aims",
    "section": "2.1 Definition",
    "text": "2.1 Definition\nRemote sensing: acquiring information from a distance(e.g. Landsat and Sentinel), achieved through sensors(Satellites, Planes, Drones, Phones and free standing on the ground or sea).\nWaves of an electromagnetic field, travel through space and carry radiant energy = Electromagnetic radiation (EMR). Waves are part of the EMR spectrum.\nEnergy carried by EMR waves = radiant energy\nEnergy per unit of time = radiant flux\nEnergy from the sun = incoming short wave radiation or shortwave radiation\nEnergy (solar power) from the sun per unit area per unit time (from electromagnetic radiation) = solar irradiance (per unit time - flux)\nEnergy leaving a surface per unit area per unit time = Exitance (emittance) (per unit time - flux)\nFlux means time.\nRayleigh = particles are very small compared to the wavelength\nMie = particles are the same size compared to the wavelength\nNon selective = particles are much larger than the wavelength\nData formats \n\\[ \\left\\{\n\\begin{array}{l}\n\\text{geosynchronous orbit (GSO) = satellite matches the Earth's rotation } \\\\\n\\text{geostationary orbit = holds same position, usually only for communications but some sensors are geostationary.}\n\\end{array}\n\\right. \\]"
  },
  {
    "objectID": "Week1.html#two-types-of-sensors",
    "href": "Week1.html#two-types-of-sensors",
    "title": "1  1. Module aims",
    "section": "2.2 Two types of sensors",
    "text": "2.2 Two types of sensors\n\n\n\n\n\n\n\npassive\nactive\n\n\n\n\nuse available energy\nenergy source for illumination\n\n\nemit nothing\nemit electromagnetic\n\n\ndetecting reflected energy from the sun\n-\n\n\nenergy in electromagnetic waves\n-\n\n\nhuman eye, camera, satellite sensor\nRadar, X-ray, LiDAR"
  },
  {
    "objectID": "Week1.html#electromagnetic-waves",
    "href": "Week1.html#electromagnetic-waves",
    "title": "1  1. Module aims",
    "section": "2.3 Electromagnetic waves",
    "text": "2.3 Electromagnetic waves\n\\[\n\\lambda = \\frac{c }{v}\n\\]  This kind of energy waves reflected by the surface."
  },
  {
    "objectID": "Week1.html#scattering-in-action",
    "href": "Week1.html#scattering-in-action",
    "title": "1  1. Module aims",
    "section": "2.4 Scattering in action",
    "text": "2.4 Scattering in action\nSunlight is scattered by particles in the atmosphere and smaller wavelengths scatter easier. Wavelength of visible light(from long to short): red, orange, yellow, green and blue.\nThus both sky and ocean seems blue in eyes for blue light has shorter wavelength and is easy to be scattered and reflected. But there’s no atmosphere on the moon so no scattering can happen, thus the moon have a black sky.\nSo clouds is a big problem in remotely sensing for it will affect the wavelength received by satellite. Use Synthetic Aperture Radar(SAR) to “see through clouds”."
  },
  {
    "objectID": "Week1.html#interacting-with-earths-surface",
    "href": "Week1.html#interacting-with-earths-surface",
    "title": "1  1. Module aims",
    "section": "2.5 Interacting with Earth’s surface",
    "text": "2.5 Interacting with Earth’s surface\nBidirectional Reflectance Distribution Function(BRDF): Change view and illumination angles.\nSAR data"
  },
  {
    "objectID": "Week1.html#resolution",
    "href": "Week1.html#resolution",
    "title": "1  1. Module aims",
    "section": "2.6 Resolution",
    "text": "2.6 Resolution\nremotely sensed data varied based on four resolution:\n\\[ \\left\\{\n\\begin{array}{l}\n\\text{Spatial} \\\\\n\\text{Spectral} \\\\\n\\text{Temporal} \\\\\n\\text{Radiometric}\n\\end{array}\n\\right. \\]\n\n2.6.1 Spectral resolution\nTake values for each wavelength across the electromagnetic spectrum to create a spectral signature thus every feature on Earth will have a unique spectral signature.\nConstrain: atmospheric window - Water vapour, ozone, carbon dioxide and atmospheric molecules block parts of the the spectrum, we can only observe where there aren’t absorbed by the atmosphere.\n\n\n\nImage 5\n\n\nsummarise, application reflection 内容摘要，数据/概念/方法（或相关概念）已应用于文献/政策或其他研究，对所呈现内容的个人反思"
  },
  {
    "objectID": "Week3.html#definition",
    "href": "Week3.html#definition",
    "title": "3  1. Summary",
    "section": "3.1 Definition",
    "text": "3.1 Definition\nregression: \\[\ny_{i} =  \\beta _{0} + \\beta_{1}x_{i} + \\epsilon _{i}\n\\] \\(\\beta_{0}\\) is the intercept(when \\(x=0\\), \\(y = \\beta{0}\\)).\n\\(\\beta_{1}\\) is the ‘slope’.\n\\(\\epsilon_{i}\\) is a random error."
  },
  {
    "objectID": "Week3.html#corrections",
    "href": "Week3.html#corrections",
    "title": "3  Data processing",
    "section": "3.2 1.1 Corrections",
    "text": "3.2 1.1 Corrections\nGeometric\n\n\n\nVisualization of geometric correction\nsolution:\nStep 1: model coordinates to give geometric transformation coefficients.\nStep 2: use distorted x or y as the dependent or independent for linear regression.\n\\[\n\\left\\{\\begin{matrix}\nx = a_{0} + a_{1}x^{i} + a_2y^i + \\epsilon_i \\\\\ny = b_{0} + b_{1}x^{i} + b_2y^i + \\epsilon_i\n\\end{matrix}\\right.\n\\] from positions in original image(\\(x^i\\) and \\(y^i\\)) to rectified map(\\(x\\) and \\(y\\)).\n\nStep 3: minimize the RMSE.\n\n3.2.1 Atmospheric\n\n\n3.2.2 Orthorectification/ Topographic correction\n\n\n3.2.3 Radiometric"
  },
  {
    "objectID": "Week3.html#data-joining-and-enhancement",
    "href": "Week3.html#data-joining-and-enhancement",
    "title": "3  Data processing",
    "section": "3.2 1.2 Data joining and enhancement",
    "text": "3.2 1.2 Data joining and enhancement\n\n3.2.1 Feathering\n\n\n3.2.2 Image enhancement\n\n\n\n\nNV5 Geospatial Software. n.d. “MosaicSeamless Documentation.” https://www.nv5geospatialsoftware.com/docs/MosaicSeamless.html."
  },
  {
    "objectID": "Week2.html#section",
    "href": "Week2.html#section",
    "title": "2  Summary",
    "section": "2.1 ",
    "text": "2.1"
  },
  {
    "objectID": "Week4.html#example-applications",
    "href": "Week4.html#example-applications",
    "title": "4  1. Summary",
    "section": "4.1 Example applications",
    "text": "4.1 Example applications\nUrban expansion Sensor: Landsat\nLULC(land use and land cover)\nUrban green spaces\nDisaster response/preparedness\nSAR(Synthetic Aperture Radar) Amplitude(backscatter) and phase\nInSAR\nMonitoring forests + illegal logging pre-processing creating metrics definition of feature space used for basci classification training data(machine learning) classification(supervised or unsupervised)(random forest)(need hyperparameters like DBSCAN)\nDroughts\nForest fires"
  },
  {
    "objectID": "Week4.html#policy-challenges",
    "href": "Week4.html#policy-challenges",
    "title": "4  1. Summary",
    "section": "4.2 Policy challenges",
    "text": "4.2 Policy challenges"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "5  bibliography: references.bib",
    "section": "",
    "text": "6 Introduction\nThis is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\nReferences\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "6  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "0023RS",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Week4.html",
    "href": "Week4.html",
    "title": "4  Summary",
    "section": "",
    "text": "Edinburgh, located in southeastern Scotland, is the capital city of Scotland. It follows the development characteristic of most English cities, which is to gradually expand based on the original Old Town and develop the New Town (although it is called New Town, it is more than three hundred years old). Today, the city of Edinburgh develops outwards with the old Town and the New town as the core, especially extending to the southwest since the barrier to northward expansion was the lake and encircling marsh. In economic terms, Edinburgh today is primarily a service provider and the second largest travel destination in the UK. At the same time, as can be seen from its published 2030 plan, the Edinburgh government hopes to develop its northern coastal natural resources for further development of tourism at this stage. It is worth mentioning that the tourism landscape development project in the north of the country has had a government-led development plan since 2000, but due to various problems such as the financial crisis and defamation charges, its planning has been continuously shelved, and has been replaced by another similar plan, but this plan is still in the initial stage of establishment, there is still a lot of room for development. In addition, another focus of the Edinburgh government is the establishment of the blue and green network of the city, which is expected to complete the establishment of the blue and green network of the entire city of Edinburgh by 2030 and achieve the goal of zero carbon emissions, so as to effectively mitigate its heat island effect.\n\n5 Application\nIf we want to get a deeper understanding of this kind of problem, you can consider some quantitative indicators related to remote sensing, which may include some data related to climate change, population activity, and land north of Edinburgh over the past few decades. For example, the impact of tourism on the heat island effect of Edinburgh, the change trend of environmental resources in the northern coastal areas, the expansion of urban area in the north of Edinburgh, the growth of population and even the change of its composition, as well as the transformation of some land uses may also reflect the problems it is currently facing. 根据以上信息可知，在未来几年中爱丁堡的环保型旅游项目很可能成为其所需要的重点项目，因此\n\n\n6 Reflection\nThis kind of research on a city’s policy is undoubtedly complicated, because it needs to consider many factors. To be honest, at the beginning I read a lot of literature but still didn’t know where to start. But from what I’ve written so far, it would be much easier to focus on just a small part of the city, especially if that part is a little more independent than the rest of the city."
  },
  {
    "objectID": "Week1.html#summary",
    "href": "Week1.html#summary",
    "title": "1  Getting started with remote sensing",
    "section": "1.2 Summary",
    "text": "1.2 Summary\n\n1.2.1 Definition\nRemote sensing: acquiring information from a distance(e.g. Landsat and Sentinel), achieved through sensors(Satellites, Planes, Drones, Phones and free standing on the ground or sea).\nWaves of an electromagnetic field, travel through space and carry radiant energy = Electromagnetic radiation (EMR). Waves are part of the EMR spectrum.\nEnergy carried by EMR waves = radiant energy\nEnergy per unit of time = radiant flux\nEnergy from the sun = incoming short wave radiation or shortwave radiation\nEnergy (solar power) from the sun per unit area per unit time (from electromagnetic radiation) = solar irradiance (per unit time - flux)\nEnergy leaving a surface per unit area per unit time = Exitance (emittance) (per unit time - flux)\nFlux means time.\nRayleigh = particles are very small compared to the wavelength\nMie = particles are the same size compared to the wavelength\nNon selective = particles are much larger than the wavelength\nData formats \n\\[ \\left\\{\n\\begin{array}{l}\n\\text{geosynchronous orbit (GSO) = satellite matches the Earth's rotation } \\\\\n\\text{geostationary orbit = holds same position, usually only for communications.}\n\\end{array}\n\\right. \\]\n\n\n1.2.2 Two types of sensors\n\n\n\n\n\n\n\npassive\nactive\n\n\n\n\nuse available energy\nenergy source for illumination\n\n\nemit nothing\nemit electromagnetic\n\n\ndetecting reflected energy from the sun\n-\n\n\nenergy in electromagnetic waves\n-\n\n\nhuman eye, camera, satellite sensor\nRadar, X-ray, LiDAR\n\n\n\n\n\n\n\n(Adamo et al. 2020)\n\n\n1.2.3 Electromagnetic waves\n\\[\n\\lambda = \\frac{c }{v}\n\\]  This kind of energy waves reflected by the surface. (Mastella 2017)\n\n\n1.2.4 Scattering in action\nSunlight is scattered by particles in the atmosphere and smaller wavelengths scatter easier. Wavelength of visible light(from long to short): red, orange, yellow, green and blue.\nThus both sky and ocean seems blue in eyes for blue light has shorter wavelength and is easy to be scattered and reflected. But there’s no atmosphere on the moon so no scattering can happen, thus the moon have a black sky.\nSo clouds is a big problem in remotely sensing for it will affect the wavelength received by satellite. Use Synthetic Aperture Radar(SAR) to “see through clouds”.\n\n\n1.2.5 Resolution\nremotely sensed data varied based on four resolution:\n\\[ \\left\\{\n\\begin{array}{l}\n\\text{Spatial} \\\\\n\\text{Spectral} \\\\\n\\text{Temporal} \\\\\n\\text{Radiometric}\n\\end{array}\n\\right. \\]\n\n\n1.2.6 Spectral resolution\nTake values for each wavelength across the electromagnetic spectrum to create a spectral signature thus every feature on Earth will have a unique spectral signature.\nConstrain: atmospheric window - Water vapour, ozone, carbon dioxide and atmospheric molecules block parts of the the spectrum, we can only observe where there aren’t absorbed by the atmosphere.\n (“Electromagnetic Spectrum Introduction” 2023)"
  },
  {
    "objectID": "Week1.html#module-aims",
    "href": "Week1.html#module-aims",
    "title": "1  Getting started with remote sensing",
    "section": "1.1 Module aims",
    "text": "1.1 Module aims\nOperationalise remotely sensed Earth observation data for informing decisions on environmental hazards arising from a changing climate, specifically in relation to urban areas and future urban sustainability."
  },
  {
    "objectID": "Week1.html#applications",
    "href": "Week1.html#applications",
    "title": "1  Getting started with remote sensing",
    "section": "1.3 Applications",
    "text": "1.3 Applications\nRemote sensing can be used widely in nature analysis and human activities as shown follows.  (GISBOX, n.d.)\nOne of the applications I am very interested in is the use of remote sensing technology to predict fires, especially mountain fires. In the American College Students Mathematical Contest in Modeling in 2021, I tried related content, such as how to use remote sensing drones to monitor fire situations and inform local fire brigades in time. This is obviously more mathematical and will use a lot of modeling knowledge, but in fact there is a lack of a lot of remote sensing data, such as the fact that there will be a relatively long period of temperature change before the open fire burns (Maffei, Lindenbergh, and Menenti 2021) and after it is extinguished(Sandal Erzurumlu and Yıldız 2024). If remote sensing technology can be used to detect such changes, it will make a great contribution to the fire prediction and prevention of its recurrence.(Gale et al. 2021)"
  },
  {
    "objectID": "Week1.html#references",
    "href": "Week1.html#references",
    "title": "1  Getting started with remote sensing",
    "section": "1.5 References",
    "text": "1.5 References\n\n\n\n\nAdamo, Nasrat, Nadhir Al-Ansari, Sabah Ali, Jan Laue, and Sven Knutsson. 2020. “Dams Safety: Review of Satellite Remote Sensing Applications to Dams and Reservoirs.” Journal of Earth Sciences and Geotechnical Engineering 11 (September): 347–438. https://doi.org/10.47260/jesge/1119.\n\n\n“Electromagnetic Spectrum Introduction.” 2023. NASA Science Mission Directorate. https://smd-cms.nasa.gov/wp-content/uploads/2023/08/ems-introduction.jpeg.\n\n\nGale, Matthew G., Geoffrey J. Cary, Albert I. J. M. Van Dijk, and Marta Yebra. 2021. “Forest Fire Fuel Through the Lens of Remote Sensing: Review of Approaches, Challenges and Future Directions in the Remote Sensing of Biotic Determinants of Fire Behaviour.” Remote Sensing of Environment 255: 112282–82.\n\n\nGISBOX. n.d. “Remote Sensing.” https://www.gisbox.ro/remote-sensing/.\n\n\nMaffei, Carmine, Roderik Lindenbergh, and Massimo Menenti. 2021. “Combining Multi-Spectral and Thermal Remote Sensing to Predict Forest Fire Characteristics.” ISPRS Journal of Photogrammetry and Remote Sensing 181: 400–412.\n\n\nMastella, André Fabiano. 2017. “AVALIAÇÃO DA ACURÁCIA TEMÁTICA PARA CLASSIFICAÇÃO DE IMAGENS DE SATÉLITE: ESTUDO DE CASO NO MUNICÍPIO DE NOVA VENEZA/SC.”\n\n\nSandal Erzurumlu, Gülden, and Nuriye Ebru Yıldız. 2024. “Determination of Fire Intensity After Forest Fire by Remote Sensing: Marmaris Case Study.” BIO Web of Conferences 85: 1041–41."
  },
  {
    "objectID": "Week1.html#refelction",
    "href": "Week1.html#refelction",
    "title": "1  Getting started with remote sensing",
    "section": "1.4 Refelction",
    "text": "1.4 Refelction\nThis lecture is really helpful to me. Honestly, I knew very little about remote sensing before this course, so I chose this course because I wanted to learn something completely new. This lecture gives me a general outline of remote sensing. I have dealt with a lot of data in my undergraduate study, and I was curious about data collection at that time. Unfortunately, how to collect raw data is not included in my undergraduate study, but from this lecture, I can see that this course is what I want to know. At the same time, it seems that this course can serve as a bridge between the data processing and modeling methods I learned before and the actual problem handling, and can transform the actual problems into more familiar data for further analysis. I hope this course will be of great help to the collection and search of data in my subsequent research."
  },
  {
    "objectID": "Week2.html",
    "href": "Week2.html",
    "title": "0023RS",
    "section": "",
    "text": "https://tongmengxie.github.io/Xaringan_slides"
  },
  {
    "objectID": "week_2.html",
    "href": "week_2.html",
    "title": "2  Presentation Ninja",
    "section": "",
    "text": "background-image: url(https://upload.wikimedia.org/wikipedia/commons/b/be/Sharingan_triple.svg)\n???\nImage credit: Wikimedia Commons\nclass: inverse, center, middle\n\n3 Get Started\n\n\n\n4 Hello World\nInstall the xaringan package from Github:\n\nremotes::install_github(\"yihui/xaringan\")\n\n–\nYou are recommended to use the RStudio IDE, but you do not have to.\n\nCreate a new R Markdown document from the menu File -&gt; New File -&gt; R Markdown -&gt; From Template -&gt; Ninja Presentation;1\n\n–\n\nClick the Knit button to compile it;\n\n–\n\nor use the RStudio Addin2 “Infinite Moon Reader” to live preview the slides (every time you update and save the Rmd document, the slides will be automatically reloaded in RStudio Viewer.\n\n.footnote[ [1] 中文用户请看这份教程\n[2] See #2 if you do not see the template or addin in RStudio. ]\n\n\n5 Hello Ninja\nAs a presentation ninja, you certainly should not be satisfied by the “Hello World” example. You need to understand more about two things:\n\nThe remark.js library;\nThe xaringan package;\n\nBasically xaringan injected the chakra of R Markdown (minus Pandoc) into remark.js. The slides are rendered by remark.js in the web browser, and the Markdown source needed by remark.js is generated from R Markdown (knitr).\n\n\n\n6 remark.js\nYou can see an introduction of remark.js from its homepage. You should read the remark.js Wiki at least once to know how to\n\ncreate a new slide (Markdown syntax* and slide properties);\nformat a slide (e.g. text alignment);\nconfigure the slideshow;\nand use the presentation (keyboard shortcuts).\n\nIt is important to be familiar with remark.js before you can understand the options in xaringan.\n.footnote[[*] It is different with Pandoc’s Markdown! It is limited but should be enough for presentation purposes. Come on… You do not need a slide for the Table of Contents! Well, the Markdown support in remark.js may be improved in the future.]\nclass: inverse, middle, center\n\n\n7 Using xaringan\n\n\n\n8 xaringan\nProvides an R Markdown output format xaringan::moon_reader as a wrapper for remark.js, and you can use it in the YAML metadata, e.g.\n---\ntitle: \"A Cool Presentation\"\noutput:\n  xaringan::moon_reader:\n    yolo: true\n    nature:\n      autoplay: 30000\n---\nSee the help page ?xaringan::moon_reader for all possible options that you can use.\n\n\n\n9 remark.js vs xaringan\nSome differences between using remark.js (left) and using xaringan (right):\n.pull-left[ 1. Start with a boilerplate HTML file;\n\nPlain Markdown;\nWrite JavaScript to autoplay slides;\nManually configure MathJax;\nHighlight code with *;\nEdit Markdown source and refresh browser to see updated slides; ]\n\n.pull-right[ 1. Start with an R Markdown document;\n\nR Markdown (can embed R/other code chunks);\nProvide an option autoplay;\nMathJax just works;*\nHighlight code with {{}};\nThe RStudio addin “Infinite Moon Reader” automatically refreshes slides on changes; ]\n\n.footnote[[*] Not really. See next page.]\n\n\n\n10 Math Expressions\nYou can write LaTeX math expressions inside a pair of dollar signs, e.g. $+$ renders \\(\\alpha+\\beta\\). You can use the display style with double dollar signs:\n$$\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i$$\n\\[\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\\]\nLimitations:\n\nThe source code of a LaTeX math expression must be in one line, unless it is inside a pair of double dollar signs, in which case the starting $$ must appear in the very beginning of a line, followed immediately by a non-space character, and the ending $$ must be at the end of a line, led by a non-space character;\nThere should not be spaces after the opening $ or before the closing $.\nMath does not work on the title slide (see #61 for a workaround).\n\n\n\n\n11 R Code\n\n# a boring regression\nfit = lm(dist ~ 1 + speed, data = cars)\ncoef(summary(fit))\n\n#               Estimate Std. Error   t value     Pr(&gt;|t|)\n# (Intercept) -17.579095  6.7584402 -2.601058 1.231882e-02\n# speed         3.932409  0.4155128  9.463990 1.489836e-12\n\ndojutsu = c('地爆天星', '天照', '加具土命', '神威', '須佐能乎', '無限月読')\ngrep('天', dojutsu, value = TRUE)\n\n# [1] \"地爆天星\" \"天照\"\n\n\n\n\n\n12 R Plots\n\npar(mar = c(4, 4, 1, .1))\nplot(cars, pch = 19, col = 'darkgray', las = 1)\nabline(fit, lwd = 2)\n\n\n\n\n\n\n\n13 Tables\nIf you want to generate a table, make sure it is in the HTML format (instead of Markdown or other formats), e.g.,\n\nknitr::kable(head(iris), format = 'html')\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n\n\n\n\n\n\n\n\n14 HTML Widgets\nI have not thoroughly tested HTML widgets against xaringan. Some may work well, and some may not. It is a little tricky.\nSimilarly, the Shiny mode (runtime: shiny) does not work. I might get these issues fixed in the future, but these are not of high priority to me. I never turn my presentation into a Shiny app. When I need to demonstrate more complicated examples, I just launch them separately. It is convenient to share slides with other people when they are plain HTML/JS applications.\nSee the next page for two HTML widgets.\n\n\nlibrary(leaflet)\nleaflet() %&gt;% addTiles() %&gt;% setView(-93.65, 42.0285, zoom = 17)\n\n\n\n\n\n\n\nDT::datatable(\n  head(iris, 10),\n  fillContainer = FALSE, options = list(pageLength = 8)\n)\n\n\n\n\n\n\n\n\n\n15 Some Tips\n\nDo not forget to try the yolo option of xaringan::moon_reader.\noutput:\n  xaringan::moon_reader:\n    yolo: true\n\n\n\n\n16 Some Tips\n\nSlides can be automatically played if you set the autoplay option under nature, e.g. go to the next slide every 30 seconds in a lightning talk:\noutput:\n  xaringan::moon_reader:\n    nature:\n      autoplay: 30000\nIf you want to restart the play after it reaches the last slide, you may set the sub-option loop to TRUE, e.g.,\noutput:\n  xaringan::moon_reader:\n    nature:\n      autoplay:\n        interval: 30000\n        loop: true\n\n\n\n\n17 Some Tips\n\nA countdown timer can be added to every page of the slides using the countdown option under nature, e.g. if you want to spend one minute on every page when you give the talk, you can set:\noutput:\n  xaringan::moon_reader:\n    nature:\n      countdown: 60000\nThen you will see a timer counting down from 01:00, to 00:59, 00:58, … When the time is out, the timer will continue but the time turns red.\n\n\n\n\n18 Some Tips\n\nThe title slide is created automatically by xaringan, but it is just another remark.js slide added before your other slides.\nThe title slide is set to class: center, middle, inverse, title-slide by default. You can change the classes applied to the title slide with the titleSlideClass option of nature (title-slide is always applied).\noutput:\n  xaringan::moon_reader:\n    nature:\n      titleSlideClass: [top, left, inverse]\n\n–\n\nIf you’d like to create your own title slide, disable xaringan’s title slide with the seal = FALSE option of moon_reader.\noutput:\n  xaringan::moon_reader:\n    seal: false\n\n\n\n\n19 Some Tips\n\nThere are several ways to build incremental slides. See this presentation for examples.\nThe option highlightLines: true of nature will highlight code lines that start with *, or are wrapped in {{ }}, or have trailing comments #&lt;&lt;;\noutput:\n  xaringan::moon_reader:\n    nature:\n      highlightLines: true\nSee examples on the next page.\n\n\n\n\n20 Some Tips\n.pull-left[ An example using a leading *:\n```r\nif (TRUE) {\n** message(\"Very important!\")\n}\n```\nOutput:\nif (TRUE) {\n* message(\"Very important!\")\n}\nThis is invalid R code, so it is a plain fenced code block that is not executed. ]\n.pull-right[ An example using {{}}:\n```{r tidy=FALSE}\nif (TRUE) {\n*{{ message(\"Very important!\") }}\n}\n```\nOutput:\n\nif (TRUE) {\n{{ message(\"Very important!\") }}\n}\n\nVery important!\n\n\nIt is valid R code so you can run it. Note that {{}} can wrap an R expression of multiple lines. ]\n\n\n\n21 Some Tips\nAn example of using the trailing comment #&lt;&lt; to highlight lines:\n```{r tidy=FALSE}\nlibrary(ggplot2)\nggplot(mtcars) + \n  aes(mpg, disp) + \n  geom_point() +   #&lt;&lt;\n  geom_smooth()    #&lt;&lt;\n```\nOutput:\n\nlibrary(ggplot2)\nggplot(mtcars) + \n  aes(mpg, disp) + \n  geom_point() +   #&lt;&lt;\n  geom_smooth()    #&lt;&lt;\n\n\n\n\n22 Some Tips\nWhen you enable line-highlighting, you can also use the chunk option highlight.output to highlight specific lines of the text output from a code chunk. For example, highlight.output = TRUE means highlighting all lines, and highlight.output = c(1, 3) means highlighting the first and third line.\n```{r, highlight.output=c(1, 3)}\nhead(iris)\n```\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nQuestion: what does highlight.output = c(TRUE, FALSE) mean? (Hint: think about R’s recycling of vectors)\n\n\n\n23 Some Tips\n\nTo make slides work offline, you need to download a copy of remark.js in advance, because xaringan uses the online version by default (see the help page ?xaringan::moon_reader).\nYou can use xaringan::summon_remark() to download the latest or a specified version of remark.js. By default, it is downloaded to libs/remark-latest.min.js.\nThen change the chakra option in YAML to point to this file, e.g.\noutput:\n  xaringan::moon_reader:\n    chakra: libs/remark-latest.min.js\nIf you used Google fonts in slides (the default theme uses Yanone Kaffeesatz, Droid Serif, and Source Code Pro), they won’t work offline unless you download or install them locally. The Heroku app google-webfonts-helper can help you download fonts and generate the necessary CSS.\n\n\n\n\n24 Macros\n\nremark.js allows users to define custom macros (JS functions) that can be applied to Markdown text using the syntax ![:macroName arg1, arg2, ...] or ![:macroName arg1, arg2, ...](this). For example, before remark.js initializes the slides, you can define a macro named scale:\nremark.macros.scale = function (percentage) {\n  var url = this;\n  return '&lt;img src=\"' + url + '\" style=\"width: ' + percentage + '\" /&gt;';\n};\nThen the Markdown text\n![:scale 50%](image.jpg)\nwill be translated to\n&lt;img src=\"image.jpg\" style=\"width: 50%\" /&gt;\n\n\n\n\n25 Macros (continued)\n\nTo insert macros in xaringan slides, you can use the option beforeInit under the option nature, e.g.,\noutput:\n  xaringan::moon_reader:\n    nature:\n      beforeInit: \"macros.js\"\nYou save your remark.js macros in the file macros.js.\nThe beforeInit option can be used to insert arbitrary JS code before remark.create(). Inserting macros is just one of its possible applications.\n\n\n\n\n26 CSS\nAmong all options in xaringan::moon_reader, the most challenging but perhaps also the most rewarding one is css, because it allows you to customize the appearance of your slides using any CSS rules or hacks you know.\nYou can see the default CSS file here. You can completely replace it with your own CSS files, or define new rules to override the default. See the help page ?xaringan::moon_reader for more information.\n\n\n\n27 CSS\nFor example, suppose you want to change the font for code from the default “Source Code Pro” to “Ubuntu Mono”. You can create a CSS file named, say, ubuntu-mono.css:\n@import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);\n\n.remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }\nThen set the css option in the YAML metadata:\noutput:\n  xaringan::moon_reader:\n    css: [\"default\", \"ubuntu-mono.css\"]\nHere I assume ubuntu-mono.css is under the same directory as your Rmd.\nSee yihui/xaringan#83 for an example of using the Fira Code font, which supports ligatures in program code.\n\n\n\n28 CSS (with Sass)\nxaringan also supports Sass support via rmarkdown. Suppose you want to use the same color for different elements, e.g., first heading and bold text. You can create a .scss file, say mytheme.scss, using the sass syntax with variables:\n$mycolor: #ff0000; \n.remark-slide-content &gt; h1 { color: $mycolor; }\n.remark-slide-content strong { color: $mycolor; }\nThen set the css option in the YAML metadata using this file placed under the same directory as your Rmd:\noutput:\n  xaringan::moon_reader:\n    css: [\"default\", \"mytheme.scss\"]\nThis requires rmarkdown &gt;= 2.8 and the sass package. You can learn more about rmarkdown and sass support in this blog post and in sass overview vignette.\n\n\n\n29 Themes\nDon’t want to learn CSS? Okay, you can use some user-contributed themes. A theme typically consists of two CSS files foo.css and foo-fonts.css, where foo is the theme name. Below are some existing themes:\n\nnames(xaringan:::list_css())\n\n [1] \"chocolate-fonts\"  \"chocolate\"        \"default-fonts\"   \n [4] \"default\"          \"duke-blue\"        \"fc-fonts\"        \n [7] \"fc\"               \"glasgow_template\" \"hygge-duke\"      \n[10] \"hygge\"            \"ki-fonts\"         \"ki\"              \n[13] \"kunoichi\"         \"lucy-fonts\"       \"lucy\"            \n[16] \"metropolis-fonts\" \"metropolis\"       \"middlebury-fonts\"\n[19] \"middlebury\"       \"nhsr-fonts\"       \"nhsr\"            \n[22] \"ninjutsu\"         \"rladies-fonts\"    \"rladies\"         \n[25] \"robot-fonts\"      \"robot\"            \"rutgers-fonts\"   \n[28] \"rutgers\"          \"shinobi\"          \"tamu-fonts\"      \n[31] \"tamu\"             \"uio-fonts\"        \"uio\"             \n[34] \"uo-fonts\"         \"uo\"               \"uol-fonts\"       \n[37] \"uol\"              \"useR-fonts\"       \"useR\"            \n[40] \"uwm-fonts\"        \"uwm\"              \"wic-fonts\"       \n[43] \"wic\"             \n\n\n\n\n\n30 Themes\nTo use a theme, you can specify the css option as an array of CSS filenames (without the .css extensions), e.g.,\noutput:\n  xaringan::moon_reader:\n    css: [default, metropolis, metropolis-fonts]\nIf you want to contribute a theme to xaringan, please read this blog post.\nbackground-image: url(https://upload.wikimedia.org/wikipedia/commons/b/be/Sharingan_triple.svg) background-size: 100px background-position: 90% 8%\n\n\n31 Sharingan\nThe R package name xaringan was derived1 from Sharingan, a dōjutsu in the Japanese anime Naruto with two abilities:\n\nthe “Eye of Insight”\nthe “Eye of Hypnotism”\n\nI think a presentation is basically a way to communicate insights to the audience, and a great presentation may even “hypnotize” the audience.2,3\n.footnote[ [1] In Chinese, the pronounciation of X is Sh /ʃ/ (as in shrimp). Now you should have a better idea of how to pronounce my last name Xie.\n[2] By comparison, bad presentations only put the audience to sleep.\n[3] Personally I find that setting background images for slides is a killer feature of remark.js. It is an effective way to bring visual impact into your presentations. ]\n\n\n\n32 Naruto terminology\nThe xaringan package borrowed a few terms from Naruto, such as\n\nSharingan (写輪眼; the package name)\nThe moon reader (月読; an attractive R Markdown output format)\nChakra (查克拉; the path to the remark.js library, which is the power to drive the presentation)\nNature transformation (性質変化; transform the chakra by setting different options)\nThe infinite moon reader (無限月読; start a local web server to continuously serve your slides)\nThe summoning technique (download remark.js from the web)\n\nYou can click the links to know more about them if you want. The jutsu “Moon Reader” may seem a little evil, but that does not mean your slides are evil.\n\nclass: center\n\n\n33 Hand seals (印)\nPress h or ? to see the possible ninjutsu you can use in remark.js.\n\n\nclass: center, middle\n\n\n34 Thanks!\nSlides created via the R package xaringan.\nThe chakra comes from remark.js, knitr, and R Markdown."
  },
  {
    "objectID": "Week2.html#the-presentation",
    "href": "Week2.html#the-presentation",
    "title": "2  Portfolio",
    "section": "2.1 The presentation",
    "text": "2.1 The presentation"
  },
  {
    "objectID": "Week3.html#summary",
    "href": "Week3.html#summary",
    "title": "3  Data processing",
    "section": "3.1 Summary",
    "text": "3.1 Summary\n\n3.1.1 Definition\n\nRadiance, refers to any radiation leaving the Earth (i.e. upwelling, toward the sensor.\nIrradiance, is used to describe downwelling radiation reaching the Earth from the sun.\nDigital number(DN)(0-255 for 8 bit ot 0-65536 for 12 bit): Intensity of the electromagnetic radiation per pixel.\nReflectance: a property of a material, typically surface reflectance(bottom of atmosphere) but also can be top of atmosphere reflectance.\nHemispherical reflectance: All of the light leaving the surface goes straight to the sensor (nothing is intercepted or at an angle)\nApparent reflectance(reflectance): create reflectance accounting for atmospheric and illumination effects. BUT this typically doesn’t deal with shadows and directional effects (e.g. viewing angles)\nMosaicking (similar to merging in GIS): feather the images together to create a seamless mosaic or images.\nSeamline: The dividing line.\n\n\n\n3.1.2 Part 1: Corrections\nTo deal with the flaws in remotely sensed images, several steps are used to correct them.\n\n\n\n\nCorrections for different problem\n\nGeometric correction\n\n\n\n\nVisualization of geometric correction\n\nStep 1: model coordinates to give geometric transformation coefficients.\nStep 2: use distorted x or y as the dependent or independent for linear regression.\n\\[\n\\left\\{\\begin{matrix}\nx = a_{0} + a_{1}x^{i} + a_2y^i + \\epsilon_i \\\\\ny = b_{0} + b_{1}x^{i} + b_2y^i + \\epsilon_i\n\\end{matrix}\\right.\n\\] from positions in original image(\\(x^i\\) and \\(y^i\\)) to rectified map(\\(x\\) and \\(y\\)).\n\nStep 3: minimize the RMSE.\nAtmospheric correction\n\n\n\n\nComparison before and after atmospheric correction\n\n\n\n3.1.3 Part 2: Data joining and enhancement\n\n3.1.3.1 Joining\n\n\n\n\nHow to join\n\n(NV5 Geospatial Software, n.d.)\n\n\n3.1.3.2 Image enhenrence\n\nContrast Enhancement: expand the range of image band using\n\nMinimum-Maximum\nPercentage Linear and Standard Deviation\nPiecewise Linear Contrast Stretch"
  },
  {
    "objectID": "Week3.html#application",
    "href": "Week3.html#application",
    "title": "3  Data processing",
    "section": "3.2 Application",
    "text": "3.2 Application\nAccording to the articles I have looked up so far, the correction technology of remote sensing has been relatively mature and widely used in various places (although it is more used for data processing before main analysis rather than see correction as a main discussion topic). For example, Gholizadeh et al. (2018) mainly studied the relationship between soil environment and species diversity through remote sensing data, but adopted many correction methods and PCA dimensionality reduction technology to conduct preliminary processing of remote sensing data.\nBut in the cutting-edge research, calibration still has its room for progress, it seems that the continued study of calibration technology is mainly reflected in two parts. One is to consider the correction in different natural environments, such as polar(Nitze et al. 2018) and desert (Ouatiki, Boudhar, and Chehbouni 2023) regions, the main goal of which is to make the application of remote sensing technology in extreme areas more stable and reliable. The other is to try to build more advanced models, which may be mainly combined with time series (Roujean, Leroy, and Deschamps 1992), machine learning (Huifang Li and Zhang 2016) or data fusion (Hamm, Atkinson, and Milton 2012) (Toutin 2011), with the main goal of better batch processing of remote sensing data in complex environments."
  },
  {
    "objectID": "Week3.html#references",
    "href": "Week3.html#references",
    "title": "3  Data processing",
    "section": "3.4 References",
    "text": "3.4 References\n\n\n\n\nGholizadeh, Hamed, John A. Gamon, Arthur I. Zygielbaum, Ran Wang, Anna K. Schweiger, and Jeannine Cavender-Bares. 2018. “Remote Sensing of Biodiversity: Soil Correction and Data Dimension Reduction Methods Improve Assessment of α-Diversity (Species Richness) in Prairie Ecosystems.” Remote Sensing of Environment 206: 240–53. https://doi.org/https://doi.org/10.1016/j.rse.2017.12.014.\n\n\nHamm, N. A. S., P. M. Atkinson, and E. J. Milton. 2012. “A Per-Pixel, Non-Stationary Mixed Model for Empirical Line Atmospheric Correction in Remote Sensing.” Remote Sensing of Environment 124: 666–78. https://doi.org/https://doi.org/10.1016/j.rse.2012.05.033.\n\n\nHuifang Li, Huanfeng Shen, Xiaojing Wang, and Liangpei Zhang. 2016. “An Efficient Multi-Resolution Variational Retinex Scheme for the Radiometric Correction of Airborne Remote Sensing Images.” International Journal of Remote Sensing 37 (5): 1154–72. https://doi.org/10.1080/01431161.2016.1145364.\n\n\nNitze, Ingmar, Guido Grosse, Benjamin M. Jones, Vladimir E. Romanovsky, and Julia Boike. 2018. “Remote Sensing Quantifies Widespread Abundance of Permafrost Region Disturbances Across the Arctic and Subarctic.” Nature Communications 9 (1): 5423. https://doi.org/10.1038/s41467-018-07663-3.\n\n\nNV5 Geospatial Software. n.d. “MosaicSeamless Documentation.” https://www.nv5geospatialsoftware.com/docs/MosaicSeamless.html.\n\n\nOuatiki, H., A. Boudhar, and A. Chehbouni. 2023. “Accuracy Assessment and Bias Correction of Remote Sensing–Based Rainfall Products over Semiarid Watersheds.” Theoretical and Applied Climatology 154: 763–80. https://doi.org/10.1007/s00704-023-04586-y.\n\n\nRoujean, Jean-Louis, Marc Leroy, and Pierre-Yves Deschamps. 1992. “A Bidirectional Reflectance Model of the Earth’s Surface for the Correction of Remote Sensing Data.” Journal of Geophysical Research: Atmospheres 97 (D18): 20455–68. https://doi.org/10.1029/92JD01411.\n\n\nToutin, Thierry. 2011. “State-of-the-Art of Geometric Correction of Remote Sensing Data: A Data Fusion Perspective.” International Journal of Image and Data Fusion 2 (1): 3–35. https://doi.org/10.1080/19479832.2010.539188."
  },
  {
    "objectID": "Week3.html#reflection",
    "href": "Week3.html#reflection",
    "title": "3  Data processing",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\nThere is no doubt that this lesson is a very important step in remote sensing. In GIS last semester I got uncorrected data and used it to draw a very bad graph (thanks to my friend for teaching me how to find the correct data!). It is foreseeable that these methods will be frequently used as a very basic part in future practice, just like writing literature review in an article. In addition, I have frequently used methods such as PCA in other courses, but I did not put them into practice in this course. I’d like to try them out later if I get the chance to see if there’s any difference between them and the apps in other classes (although I can’t quite tell the difference so far).\nBut it’s very likely that the things that are taught in this lecture will only be used as tools and I won’t study them in depth. I looked at the papers I mentioned earlier, and to be honest, the results didn’t look very good, and although the authors used very complex models, they didn’t have as significant an impact on the results as I expected."
  },
  {
    "objectID": "Week6.html#summary",
    "href": "Week6.html#summary",
    "title": "5  Google Earth Engine",
    "section": "5.1 Summary",
    "text": "5.1 Summary\n\n5.1.1 Definition\n\nGoogle Earth Engine: offer “Geospatial” processing service at scale, stores data on servers and takes the code written by prohrammer and applies it\nImage = raster, has bands\nFeature = vector, has geometry and attributes(dictionary of properties in GEE)\nImageCollection = Image stack\nFeatureColletion = Feature stack(lots of polygons)\nImage scale: means pixel resolution in GEE, is set by the output not input.\n\n\n\n5.1.2 Part 1: The set up of GEE\nGEE basic operation\n\nGEE use JavaScript\nRelating spatial data formats we have seen to GEE\nScale(resolution)\nRun codes on both the client and server side\n\n\n\n\n\nmap after scale\n\n\n\n5.1.3 Part 2: GEE in action\n\n\n\n\nscreen of GEE\n\nA significant advantage of GEE compared to other software I am familiar with before is that it is very fast and does not require much computing resources (because it is an online website). Like GIS, GEE has a lot of objects(vector, raster, feature, string, number) belongs to different classes with specific GEE functions\nThe remote sensing operations using R taught in CASA0005 GIS can basically be implemented in GEE, such as spatial operations (including join, partition statistics, filtering images or specific values) and some analysis methods (such as machine learning, classification, deep learning) and output some charts and applications. The specific code is too much not put here, you can see the ppt of this chapter or check the Internet."
  },
  {
    "objectID": "Week6.html#application",
    "href": "Week6.html#application",
    "title": "5  Google Earth Engine",
    "section": "5.2 Application",
    "text": "5.2 Application\nThe Google Earth Engine (GEE) platform offers expanded possibilities for conducting earth observation research. Launched in late 2010, it grants access to a wealth of satellite and supplementary data, alongside cloud computing capabilities and processing algorithms designed to handle large datasets with ease. (Kumar and Mutanga 2018)Its extensive data archive encompasses information from various satellites, as well as vector datasets from Geographic Information Systems (GIS), social and demographic data, weather records, digital elevation models, and climate datasets. This resource empowers scientists, independent researchers, enthusiasts, and nations to delve into this extensive repository of information for tasks such as change detection, trend mapping, and resource quantification on Earth’s surface, surpassing previous capabilities. (Mutanga and Kumar 2019)\n\n\n\n\nResource from: (Pham-Duc et al. 2023)\n\n\nThe fields of articles used GEE\n\nConsequently, the platform’s popularity has surged, reflected in a sharp increase in the number of GEE-related articles, with nearly 85% of them being published within the last three years.(Pham-Duc et al. 2023)\n\n\n\n\n\nResource from: (Pham-Duc et al. 2023)\n\n\nArticles published each year using GEE\n\nIn addition, by extracting the keywords of these words to make word clouds, it can be seen that the data observed by Landsat, Stneinel-1, MODIS and Sentinel-2 in GEE are widely used in the research.\n\n\n\n\n\nResource from: (Pham-Duc et al. 2023)\n\n\nWorld cloud\n\nIn the field of Earth and Planetary Science, I am very interested in land use analysis (Floreano and Moraes Luzia 2021)and crop production analysis(Venkatappa et al. 2021), and the ideas of these two articles happen to be very similar. They all observe changes in the development of remote sensing data by using GEE to visualize it over different time periods, then consider the reasons for the changes, and then consider the impact of some particular event on their research objectives. For example, Venkatappa et al. (2021) considers the impact of drought and flood on crop production while Floreano and Moraes Luzia (2021) considers whether the promulgation of some policies has an impact on land use."
  },
  {
    "objectID": "Week6.html#reflection",
    "href": "Week6.html#reflection",
    "title": "5  Google Earth Engine",
    "section": "5.3 Reflection",
    "text": "5.3 Reflection\nGEE is really a popular platform for remote sensing, probably due to the fact that its operating system is very simple and friendly to most people. Although I was prepared to find many articles before I looked up the application of GEE, the number of articles related to it still shocked me. What is even more shocking to me is that there is even a special category of articles on statistics and analysis of these GEE-related articles, and these articles are published almost every year. From discussing the potential and trend of GEE (Gorelick et al. 2017; Kumar and Mutanga 2018) to systematically reviewing these texts nearly every year(Zhao et al. 2021; Tamiminia et al. 2020), and even considering the impact of major events such as covid-19(Pérez-Cutillas et al. 2023), it can be seen that GEE has undoubtedly become a hot spot in remote sensing in recent years.\nWhat worried me was that I hadn’t worked with JavaScript before, and this week’s code, while seemingly basic, still took me a lot of time to understand and find bugs. It seems that GEE will still be needed in the next few weeks of the course, and hopefully I can learn more about JavaScript and how it is used in GEE in the future."
  },
  {
    "objectID": "Week6.html#references",
    "href": "Week6.html#references",
    "title": "5  Google Earth Engine",
    "section": "5.4 References",
    "text": "5.4 References\n\n\n\n\nFloreano, Isabela X., and Alice F. de Moraes Luzia. 2021. “Land Use/Land Cover (LULC) Analysis (2009–2019) with Google Earth Engine and 2030 Prediction Using Markov-CA in the Rondônia State, Brazil.” Environmental Monitoring and Assessment 193 (4). https://www.proquest.com/scholarly-journals/land-use-cover-lulc-analysis-2009-2019-with/docview/2506944671/se-2.\n\n\nGorelick, Noel, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David Thau, and Rebecca Moore. 2017. “Google Earth Engine: Planetary-Scale Geospatial Analysis for Everyone.” Remote Sensing of Environment 202: 18–27. https://doi.org/https://doi.org/10.1016/j.rse.2017.06.031.\n\n\nKumar, Lalit, and Onisimo Mutanga. 2018. “Google Earth Engine Applications Since Inception: Usage, Trends, and Potential.” Remote Sensing 10 (10). https://doi.org/10.3390/rs10101509.\n\n\nMutanga, Onisimo, and Lalit Kumar. 2019. “Google Earth Engine Applications.” Remote Sensing 11 (5). https://www.proquest.com/scholarly-journals/google-earth-engine-applications/docview/2303995980/se-2.\n\n\nPérez-Cutillas, Pedro, Alberto Pérez-Navarro, Carmelo Conesa-García, Demetrio Antonio Zema, and Jesús Pilar Amado-Álvarez. 2023. “What Is Going on Within Google Earth Engine? A Systematic Review and Meta-Analysis.” Remote Sensing Applications: Society and Environment 29: 100907. https://doi.org/https://doi.org/10.1016/j.rsase.2022.100907.\n\n\nPham-Duc, B., H. Nguyen, H. Phan, and et al. 2023. “Trends and Applications of Google Earth Engine in Remote Sensing and Earth Science Research: A Bibliometric Analysis Using Scopus Database.” Earth Science Informatics 16: 2355–71. https://doi.org/10.1007/s12145-023-01035-2.\n\n\nTamiminia, Haifa, Bahram Salehi, Masoud Mahdianpari, Lindi Quackenbush, Sarina Adeli, and Brian Brisco. 2020. “Google Earth Engine for Geo-Big Data Applications: A Meta-Analysis and Systematic Review.” ISPRS Journal of Photogrammetry and Remote Sensing 164: 152–70. https://doi.org/https://doi.org/10.1016/j.isprsjprs.2020.04.001.\n\n\nVenkatappa, Manjunatha, Nophea Sasaki, Phoumin Han, and Issei Abe. 2021. “Impacts of Droughts and Floods on Croplands and Crop Production in Southeast Asia – an Application of Google Earth Engine.” Science of The Total Environment 795: 148829. https://doi.org/https://doi.org/10.1016/j.scitotenv.2021.148829.\n\n\nZhao, Qiang, Le Yu, Xuecao Li, Dailiang Peng, Yongguang Zhang, and Peng Gong. 2021. “Progress and Trends in the Application of Google Earth and Google Earth Engine.” Remote Sensing 13 (18). https://doi.org/10.3390/rs13183778."
  },
  {
    "objectID": "Week7.html#summary",
    "href": "Week7.html#summary",
    "title": "6  Classification I",
    "section": "6.1 Summary",
    "text": "6.1 Summary\n\n6.1.1 Definition\nPRODES and DETER = annual data or 250m resolution\nFeature space = scattergram of two bands (or things that have been made into bands)\ninductive learning = given context we can use experience to make judgement\nIn expert system:\n\nKnowledge Base = Rules of thumb, not always correct\nInference Engine = Process of reaching a conclusion and the expert system is implemented\n\nIn machine learning:\n\nMachine learning = science of computer modeling of learning process\nAlgorithmic approach = code to solve a solution\n\nIn classification and regression tree:\n\nOverfitting = leaf with just one person or one pixel value\nBias = difference between predicted value and true value, means the model is oversimplified\nVariance = variability of model for a given point, if it’s large than means the model does not genearlise well\nPattern vector = all the band values per pixel (could include texture etc)\n\n\n\n6.1.2 Part 1: Review of how classified data is used\n\n\n\n\n\n\n\nproblems\nsensors\n\n\n\n\nurban expansion\nLandsat\n\n\nAir pollution and LULC\nSentinel-3 & Sentinel-5\n\n\nUrban green spaces\nDifferent sensors used for different mapping purposes, but can be mixed\n\n\nMonitoring forests + illegal logging\nLandsat (2000 to 2012)\n\n\nForest fires\nLandsat TM 1984\n\n\n\n\n\n6.1.3 Part 2: How to classify remotely sensed data\nPast: Expert System\n\n\n\n\n(Quora 2016)\n\nNow: Machine Learning\nA search through all the data to explain the input data and can be used on new input data.\nClassification and regression trees (CART)\n\\[\\begin{cases}\n  & \\text{dicision tree: classify data into two or more discrete categories } \\\\\n  & \\text{regression trees: predict continuous dependent variable }\n\\end{cases}\\]\nIn a regression tree, if the result does not conform to linear regression, we can divide it into parts based on the threshold and calculate the sum of residuals squares, then use the best sum of residuals squares for all variables as roots and different values as leaves.\nHowever, in practical applications, leaves may be discrete and continuous mixed together, so we use gini impurity to treat them so that they can be combined for identification.\nOverfitting\nBut sometimes, the model may facing the problem called overfitting:\n\\[\n\\text{best model}\n\\begin{cases}\n  & \\text{low bias = model the real relationship} \\\\\n  & \\text{low variability = consistent predictions between datasets}\n\\end{cases}\n\\]\nThe solution to this problem:\n\nLimit how trees grow (e.g. a minimum number of pixels in a leaf, 20 is often used)\nWeakest link pruning (with tree score):\n\nuse one less leaf, remove a leaf = sub-tree, SSR will get larger = termed PRUNING or cost complexity pruning\nSum for the tree\nTree score = SSR + tree penalty (alpha) * T (number of leaves)…lower better..\n\n\nRandom forest - Many better than one(Grow many classification decision trees)\n\nUse many trees, have each tree give a result, and then vote on the result\nBootstrapping (re-sampling by replacement data to make a decision = bagging\nFor each tree about 70% of the training data is used in the bootstrap, 30% is left out of the bag (OOB)\n\nMaximum likelihood\nUse probability to classify, it’s the data based with most probably to have the values in our pixel\nSupport Vector Machine (SVM)\nA linear binary classifier - like logistic regression.\nIt finds the divide in the data (e.g. classes) and places a line at the division from the closest points.\nApply to imagery\n\\[\\begin{cases}\n  & \\text{Supervised: Put known label onto new data(e.g. machine learning)} \\\\\n  & \\text{Unsupervised: Categorize data with labels that are not known in advance(e.g. k-means)}\n\\end{cases}\\]"
  },
  {
    "objectID": "Week7.html#application",
    "href": "Week7.html#application",
    "title": "6  Classification I",
    "section": "6.2 Application",
    "text": "6.2 Application\nAt present, classifier-related technologies have been widely used in image recognition in remote sensing. Considering only the application of land cover, there are many articles, which seem to be mainly divided into two categories, on the one hand, plaints cover(John Nay and Gilligan 2018; Stojanova et al. 2010), and on the other hand, remote sensing images of urban space(Ding et al. 2022).\nLooking at the articles related to this, it can be seen that the development history of its entire discipline is roughly as follows:\nWhen the remote sensing related to classifiers was just becoming popular, around the end of last century and the beginning of this century, researchers began to try to combine the two fields, such as Crawford, Tuia, and Yang (2013) using SVM and Friedl and Brodley (1997) using decision tree to process remote sensing images, and these papers generally focused on how to clean the initial data and model training, that is, the most basic model building and analysis process.\nLater, as more and more people pay attention to this field, more authors begin to compare the processing effects of different machine learning methods and try to put forward suggestions for improvement. In Ramezan et al. (2021)’s article, random forest was a good classifier for large-area land-cover classifications, while Park et al. (2021) thinks XGBoost is a better model.\nIn recent years, deep learning has attracted the attention of scholars in this direction. The most basic CNN convolution(Kang et al. 2021), JDR based on CNN and MLP(Zhang et al. 2019), DBN (Lv et al. 2015)and other models have been considered and optimized in this direction. In fact, the results of these models are generally better than traditional machine learning."
  },
  {
    "objectID": "Week7.html#reflection",
    "href": "Week7.html#reflection",
    "title": "6  Classification I",
    "section": "6.3 Reflection",
    "text": "6.3 Reflection\nThere is no doubt that the use of machine learning to distinguish remote sensing images is undoubtedly a very important part of remote sensing. Unfortunately, it seems that the problem related to deep learning was not mentioned in the lecture, although it is a hot topic in recent years. Among the research fields I mentioned before, I am most interested in the research topic of Ding et al. (2022). Through the analysis of remote sensing images in different periods, they clearly reflect the relationship between natural environment and urban expansion, which is undoubtedly one of the important roles of remote sensing in urban analysis.\n\n\n\n\nThe land cover maps in Shenzhen from 1979 to 2022.\n\nIn addition, this paper also reflects the relationship between different areas within the city through remote sensing images, and this discussion of taking into account social development and natural environment is, in my opinion, the mainstream direction in the coming decades.\n\n\n\n\nThe relationship between transportation and urban land increase from 1979 to 2022. (a)The road network overland on the map of urban land expansion. (b) The transportation distance histogram for urban land increase."
  },
  {
    "objectID": "Week7.html#references",
    "href": "Week7.html#references",
    "title": "6  Classification I",
    "section": "6.4 References",
    "text": "6.4 References\n\n\n\n\nCrawford, Melba M., Devis Tuia, and Hsiuhan Lexie Yang. 2013. “Active Learning: Any Value for Classification of Remotely Sensed Data?” Proceedings of the IEEE 101 (3): 593–608. https://doi.org/10.1109/JPROC.2012.2231951.\n\n\nDing, Kai, Yidu Huang, Chisheng Wang, Qingquan Li, Chao Yang, Fang Xu, Ming Tao, Renping Xie, and Ming Dai. 2022. “Time Series Analysis of Land Cover Change Using Remotely Sensed and Multisource Urban Data Based on Machine Learning: A Case Study of Shenzhen, China from 1979 to 2022.” Remote Sensing 14 (22): 5706. https://www-proquest-com.libproxy.ucl.ac.uk/scholarly-journals/time-series-analysis-land-cover-change-using/docview/2739456190/se-2.\n\n\nFriedl, M. A., and C. E. Brodley. 1997. “Decision Tree Classification of Land Cover from Remotely Sensed Data.” Remote Sensing of Environment 61 (3): 399–409. https://doi.org/https://doi.org/10.1016/S0034-4257(97)00049-7.\n\n\nJohn Nay, Emily Burchfield, and Jonathan Gilligan. 2018. “A Machine-Learning Approach to Forecasting Remotely Sensed Vegetation Health.” International Journal of Remote Sensing 39 (6): 1800–1816. https://doi.org/10.1080/01431161.2017.1410296.\n\n\nKang, Jian, Ruben Fernandez-Beltran, Puhong Duan, Sicong Liu, and Antonio J. Plaza. 2021. “Deep Unsupervised Embedding for Remotely Sensed Images Based on Spatially Augmented Momentum Contrast.” IEEE Transactions on Geoscience and Remote Sensing 59 (3): 2598–2610. https://doi.org/10.1109/TGRS.2020.3007029.\n\n\nLv, Qi, Yong Dou, Xin Niu, Jiaqing Xu, Jinbo Xu, and Fei Xia. 2015. “Urban Land Use and Land Cover Classification Using Remotely Sensed SAR Data Through Deep Belief Networks.” Journal of Sensors 2015. https://www-proquest-com.libproxy.ucl.ac.uk/scholarly-journals/urban-land-use-cover-classification-using/docview/1702625853/se-2.\n\n\nPark, J. et al. 2021. “Assessment of Machine Learning Algorithms for Land Cover Classification Using Remotely Sensed Data.” Sensors and Materials 33 (11): 3885–3902. https://www.jstage.jst.go.jp/article/sensmat/33/11/33_3885/_article.\n\n\nQuora. 2016. “What Is a Knowledge-Based System in the Context of Artificial Intelligence?” https://www.quora.com/What-is-a-knowledge-based-system-in-the-context-of-artificial-intelligence.\n\n\nRamezan, Christopher A., Timothy A. Warner, Aaron E. Maxwell, and Bradley S. Price. 2021. “Effects of Training Set Size on Supervised Machine-Learning Land-Cover Classification of Large-Area High-Resolution Remotely Sensed Data.” Remote Sensing 13 (3). https://doi.org/10.3390/rs13030368.\n\n\nStojanova, Daniela, Panče Panov, Valentin Gjorgjioski, Andrej Kobler, and Sašo Džeroski. 2010. “Estimating Vegetation Height and Canopy Cover from Remotely Sensed Data with Machine Learning.” Ecological Informatics 5 (4): 256–66. https://doi.org/https://doi.org/10.1016/j.ecoinf.2010.03.004.\n\n\nZhang, Ce, Isabel Sargent, Xin Pan, Huapeng Li, Andy Gardiner, Jonathon Hare, and Peter M. Atkinson. 2019. “Joint Deep Learning for Land Cover and Land Use Classification.” Remote Sensing of Environment 221: 173–87. https://doi.org/https://doi.org/10.1016/j.rse.2018.11.014."
  },
  {
    "objectID": "Week8.html#summary",
    "href": "Week8.html#summary",
    "title": "7  Classification II",
    "section": "7.1 Summary",
    "text": "7.1 Summary\n\n7.1.1 Definition\nOBIA: - Superpixels = Consider shapes based on the similarity (homogeneity) or difference (heterogeneity) of the cells\n\n\n7.1.2 Part 1: Land Cover Classification\nObject based image analysis (OBIA)\nMost common method: SLIC (Simple Linear Iterative Clustering)\n\nwork out spatial distance (from regular points on the image to centre of pixel) and colour difference\nPackage: SegOptim\n\nSub pixel analysis\nRemotely sensed images often present a mix of both pure and mixed pixels. Traditional hard classification methods assign mixed pixels to the class with the highest coverage or probability, inevitably leading to information loss. To address this issue, soft classification techniques have been introduced. These methods allocate pixel fractions to the corresponding land cover classes based on the area represented within a pixel. However, this approach does not provide information about the spatial distribution of these fractions within the pixel.\nAtkinson (1997) proposed a solution by suggesting the spatial assignment of fractions to sub-pixels within each pixel. This divides every pixel into a predefined number of sub-pixels, enabling a more detailed representation of lower-resolution pixels. Sub-pixel mapping algorithms have been applied in various forms and on fraction images with different spatial resolutions. However, assessing the accuracy of these algorithms poses challenges due to the lack of high-resolution ground truth imagery.\nTo overcome this limitation, high-resolution reference classifications are degraded to generate artificial fraction images. These artificial fraction images are then used as input for the sub-pixel mapping process, with the original reference image serving as ground truth. This approach facilitates the accuracy assessment of sub-pixel mapping algorithms. This study aims to utilize identical reference images across different sub-pixel mapping techniques, enabling a comparative evaluation of their performance.\nEXAMPLE:\n\n\n\nband\nwater\nvegeration\nsoil\n\n\n\n\n3\n13\n22\n70\n\n\n4\n5v\n80\n60\n\n\n\n\\[\\begin{align*}\n\\begin{bmatrix}\n\\text{band } 3\\\\\n\\text{band } 4\\\\\n\\text{sum to } 1\n\\end{bmatrix} &= \\begin{bmatrix}\np_{\\text{water}} & p_{\\text{veg}} & p_{\\text{soil}}\\\\\np_{\\text{water}} & p_{\\text{veg}} & p_{\\text{soil}}\\\\\n1 & 1 & 1\n\\end{bmatrix}\\begin{bmatrix}\nf_{\\text{water}} \\\\\nf_{\\text{veg}}\\\\\nf_{\\text{soil}}\n\\end{bmatrix}\n\\end{align*}\\]\nStep 1: take the inverse matrix of endmembers\n\\[\\begin{align*}\n\\begin{bmatrix}\n13 & 22 & 70\\\\\n5 & 80 & 60\\\\\n1 & 1 & 1\n\\end{bmatrix} \\to \\begin{bmatrix}\n-0.0053 & -0.0127& 1.1322\\\\\n-0.0145& 0.0150 & 0.1137\\\\\n0.0198 & -0.0024 & -0.2460\n\\end{bmatrix}\n\\end{align*}\\]\nStep 2:  solve if band 3 = 25 and band 4 = 57\n\\[\\begin{align*}\n\\begin{bmatrix}\nf_{\\text{water}} \\\\\nf_{\\text{veg}}\\\\\nf_{\\text{soil}}\n\\end{bmatrix} &= \\begin{bmatrix}\n-0.0053 & -0.0127& 1.1322\\\\\n-0.0145& 0.0150 & 0.1137\\\\\n0.0198 & -0.0024 & -0.2460\n\\end{bmatrix}\\begin{bmatrix}\n25 \\\\\n57\\\\\n1\n\\end{bmatrix}\n\\end{align*}\\]\nThus get:\n\\[\\begin{bmatrix}\n0.27 \\\\\n0.61\\\\\n0.11\n\\end{bmatrix}\\]\n27% water, 61% vegetation and 11% soil\n\n\n7.1.3 Part 2: Accuracy\n\n\n\n\nConfusion matrix\n\nwiki:confusion_matrix\n\nKappa\n\n\\(k = \\frac{p_{o}-p_{e}}{1 - p_e}\\)\n\\(p_o\\): the propotyion of cases correctly classified(accuracy) \\(=\\frac{TP+TN}{TP+TN+FP+FN}\\)\n\\(p_e\\): expected cased correctly classified by chance\n\nF1-score\n\n\\(F1 = \\frac{2*Precesion*Recall}{Precision+Recall}\\) or\n\\(F1 = \\frac{TP}{TP+\\frac{1}{2}*(FP+FN)}\\)\nvalues from 0 to 1, larger F1-score, better performance.\n\nROC curve\n\n\\(True\\ positive\\ rate = \\frac{TP}{TP+FN}\\)\n\\(False\\ positive\\ rate = \\frac{FP}{FP+TN}\\)\nGoal: Maximise true positives and minimise false positives\nArea Under the ROC Curve (AUC, or AUROC): Perfect value will be 1, random will be 0.5\n\n\n\n\n7.1.4 Cross Validation\nDividing the data set into the training set and the test set, using only one data set as the test set and all the other data as the training set, repeats this step N times (N is the number of data in the data set).\n\n\n\n\n(Bliem 2020)\n\nSpatial cross validation: same as cross validation but with clustering to the folds\n\n\n\n\n(GeoCompX contributors 2018)"
  },
  {
    "objectID": "Week8.html#application",
    "href": "Week8.html#application",
    "title": "7  Classification II",
    "section": "7.2 Application",
    "text": "7.2 Application\nThe various accuracy tests we spent a large part of the lecture talking about are actually some fixed methods for scoring models. They are used to analyze the results of almost all models. At the same time, they have little room for research.\nAs for spatial cross-validation, many papers will apply this validation method to reduce the influence of spatial dependence structure between data on fitting and prediction ability.Especially in recent years, it seems that the effect of spatial dependence on model results has been noticed by more and more researchers.(Deppner and Cajias 2024; Beigaitė, Mechenich, and Žliobaitė 2022; Allen, Kim, and Yang 2020) Before looking for relevant materials, I thought that this method was a verification method similar to confusion matrix. In fact, to my surprise, some scholars are still considering how to improve this method.\nLe Rest et al. (2014)’s research assesses an alternative method employing a spatial cross-validation technique. While this technique is typically utilized for model assessment, it can also yield valuable insights for variable selection in the presence of spatial autocorrelation. The study proposes the use of a specific form of spatial cross-validation known as spatial leave-one-out (SLOO), which offers a criterion akin to the AIC when spatial autocorrelation is absent. SLOO exclusively computes non-spatial models and employs a threshold distance (equivalent to the range of RSA) to ensure spatial independence for each omitted point. The study initially conducts simulations to compare SLOO’s performance with that of AIC. It proves particularly advantageous when the range of RSA is limited, a characteristic often observed in spatial tools. Consequently, SLOO emerges as a promising approach for selecting pertinent variables from a wide range of ecological spatial datasets."
  },
  {
    "objectID": "Week8.html#reflection",
    "href": "Week8.html#reflection",
    "title": "7  Classification II",
    "section": "7.3 Reflection",
    "text": "7.3 Reflection\nI found the exploration of spatial cross-validation particularly intriguing. While I initially perceived it as a standard validation method, Le Rest et al. (2014)’s research introduced me to the concept of spatial leave-one-out (SLOO) cross-validation. This innovative approach, which accounts for spatial autocorrelation, showed promise in variable selection for ecological spatial datasets. It was fascinating to learn about ongoing efforts to refine and improve spatial validation techniques, highlighting the dynamic nature of research in this field."
  },
  {
    "objectID": "Week8.html#references",
    "href": "Week8.html#references",
    "title": "7  Classification II",
    "section": "7.4 References",
    "text": "7.4 References\n\n\n\n\nAllen, D., A. Y. Kim, and J. Yang. 2020. “A Permutation Test and Spatial Cross-Validation Approach to Assess Models of Interspecific Competition Between Trees.” PLoS ONE 15 (3): e0229930. https://link-gale-com.libproxy.ucl.ac.uk/apps/doc/A617101249/AONE?u=ucl_ttda&sid=bookmark-AONE&xid=489db22e.\n\n\nAtkinson, Peter M. 1997. “Innovations in GIS 4 Chap. 12.” In Taylor & Francis, 166–80. London, U.K.\n\n\nBeigaitė, Rita, Michael Mechenich, and Indrė Žliobaitė. 2022. “Spatial Cross-Validation for Globally Distributed Data.” In Discovery Science, edited by Poncelet Pascal and Dino Ienco, 127–40. Cham: Springer Nature Switzerland.\n\n\nBliem, Christian. 2020. “Estimation of the (Re-)integration Likelihood into the Austrian Labour Market: A Random Forest Approach (Christian Bliem, 2020).” PhD thesis. https://doi.org/10.13140/RG.2.2.33476.96648.\n\n\nDeppner, J., and M. Cajias. 2024. “Accounting for Spatial Autocorrelation in Algorithm-Driven Hedonic Models: A Spatial Cross-Validation Approach.” J Real Estate Finan Econ 68: 235–73. https://doi.org/10.1007/s11146-022-09915-y.\n\n\nGeoCompX contributors. 2018. “Spatial Cross-Validation.” https://r.geocompx.org/spatial-cv.html.\n\n\nLe Rest, K., D. Pinaud, P. Monestiez, J. Chadoeuf, and V. Bretagnolle. 2014. “Spatial Leave-One-Out Cross-Validation for Variable Selection in the Presence of Spatial Autocorrelation.” Global Ecology and Biogeography 23 (7/8): 811–20. http://www.jstor.org/stable/24034076."
  },
  {
    "objectID": "Week9.html#summary",
    "href": "Week9.html#summary",
    "title": "8  Data processing",
    "section": "8.1 Summary",
    "text": "8.1 Summary\nIn today’s learning diary, I’ve chosen to write about topics related to temperature, simply because I’m more interested in it. In addition, I think it is also a very important part of remote sensing, with more than 700 articles published in the field of remote sensing surface temperature in 2020 alone.\n\n\n\n\nPublication statistics in the field of remote sensing-based LST, 2000-2020\n\n\n(Li et al. 2023)\n\nDefinition of Satellite-Derived LST\nLST is the thermodynamic temperature of a thin layer in the interface between soil, vegetation or other surface components and the atmosphere. It reflects how hot or cold is the Earth’s surface would feel to the touch.\n\n\n\n\nSchematic of the land surface in remote sensing\n\n\n(Li et al. 2023)\n\n\\(T_{si}\\): surface temperature\n\\(\\varepsilon_{i}\\): emissivity\n\\(\\alpha_{i}\\): projected area weight for the ith visible component\n\\(\\theta_{v}\\): VZA\n\\(\\phi_{v}\\): viewing azimuth angle\nFor a given satellite sensor, the radiometric temperature can be formulated as follows(Chandrasekhar 2013; P. Dash and Fischer 2002; Prata et al. 1995): \\[\nT_{\\mathrm{s}}\\left(\\theta_{v}, \\varphi_{v}\\right)=B_{\\lambda}^{-1}\\left[\\frac{R_{\\lambda}\\left(\\theta_{v}, \\varphi_{v}\\right)-R_{a t_{\\lambda} \\uparrow}\\left(\\theta_{v}, \\varphi_{v}\\right)-\\tau_{\\lambda}\\left(\\theta_{v}, \\varphi_{v}\\right)\\left[1-\\varepsilon_{\\lambda}\\left(\\theta_{v}, \\varphi_{v}\\right)\\right] R_{a t_{\\lambda} \\downarrow}}{\\tau_{\\lambda}\\left(\\theta_{v}, \\varphi_{v}\\right) \\varepsilon_{\\lambda}\\left(\\theta_{v}, \\varphi_{v}\\right)}\\right]\n\\] \\(T_s\\): radiometric temperature\n\\(\\theta_v\\): viewing zenith angle(VZA)\n\\(B_{\\lambda }^{-1}\\): inverse function of Planck’s law\n\\(R_\\lambda\\), \\(R_{at_{\\lambda}\\uparrow}\\) and \\(_{at_{\\lambda}\\downarrow}\\): at-sensor observed radiance, upward atmospheric radiance, and downward atmospheric radiance\n\\(\\tau_\\lambda\\): channel atmospheric transmittance\n\\(\\varepsilon_\\lambda\\): channel land surface emissivity(LSE)\n\nHeat Island\nBuildings, roads, and other man-made structures have a higher capacity to absorb and release solar radiation compared to natural features like forests and bodies of water. Consequently, urban areas, characterized by dense concentrations of such structures and limited green spaces, tend to exhibit higher temperatures than surrounding regions, forming what is known as urban heat islands (UHI). The proliferation of urban populations, coupled with the exacerbation of UHI effects and the escalating frequency and duration of heatwaves due to climate change, raise significant concerns regarding the heightened health risks faced by vulnerable urban populations during extreme heat events.\nRemote sensing technology offers a valuable tool for monitoring the dynamics of UHI phenomena over time on a global scale. Thermal mapping derived from satellite imagery enables the tracking of land surface temperature (LST) variations, while optical data captures changes in land use and cover, aiding in the estimation of air temperatures across different urban zones. Once UHIs are identified and mapped, integrating socioeconomic data such as population demographics and health indicators into heat vulnerability indices (HVI) facilitates the identification of at-risk communities and informs targeted interventions aimed at mitigating the adverse health impacts associated with heatwaves.\n\n\n\n\nLand surface temperature (in K) image of Shenzhen, China\n\n\n(Wang et al. 2019)"
  },
  {
    "objectID": "Week9.html#application",
    "href": "Week9.html#application",
    "title": "8  Data processing",
    "section": "8.2 Application",
    "text": "8.2 Application\nUnderstanding the urban heat island effect and finding ways to mitigate it has undoubtedly been the focus of environmental research in recent years. I thought I knew a lot about the heat island effect because it is, after all, one of the major problems facing China’s megacities, but the articles I found about it really surprised me, because it seems that the main factors are not just the human actions I assumed. Firstly, it is mentioned in the analysis of heat island effect that urban heat waves are transient and obviously influenced by natural environment, and high-resolution data in time and space are needed to study them (Buo, Sagris, and Jaagus 2023). So a lot of research has focused on the seasonal heat island effect (Paschalis et al. 2021-06) or the daytime versus night heat island effect(Arellano and Roca 2021) or the geographical location of the city as the heat island effect(especially humid and arid regions)(Zhang et al. 2022-07-01).\nAfter learning this knowledge, I began to wonder what factors have the greatest influence on the heat island effect, and fortunately, there are some scholars who are also interested in this question:\nImhoff et al. (2010) conducted a spatial analysis using Landsat TM-based NLCD 2001 data for impervious surface area (ISA) and MODIS data averaged over three years (2003–2005) for land surface temperature (LST) to study the urban heat island (UHI) skin temperature amplitude and its correlation with development intensity, size, and ecological setting in 38 major cities across the continental United States. Their findings revealed that the ecological context significantly influences the amplitude of summer daytime UHI, with the largest differences observed for cities situated in temperate broadleaf and mixed forest biomes. ISA was identified as the primary driver for temperature increase, explaining 70% of the total variance in LST across all cities. On average, urban areas were notably warmer than non-urban fringes by 2.9 °C, except for cities in arid and semiarid climates. The UHI amplitude exhibited seasonal asymmetry, with a higher difference in summer (4.3 °C) compared to winter (1.3 °C). In desert environments, the response of LST to ISA displayed a unique “U-shaped” horizontal gradient. The findings suggest a possible heat sink effect in certain cities. Overall, the study highlights that UHI amplitude increases with city size and varies seasonally across different biomes, particularly in forested ecosystems, indicating potential implications for residential energy consumption during summer.\n\n\n\n\nA&B show typical layout, C shows the cities and their biomes used in this article\n\nWhat is more interesting is that I used to think that vegetation could slow down the heat island effect because of its ability to absorb carbon dioxide, etc. In Park, Guldmann, and Liu (2021)’s article, it is actually due to the fact that trees can provide shelter for cities, thus reducing the formation of heat waves, which shows how sensitive heat waves are to the environment, so it really needs high-precision sensors to fully analyze them."
  },
  {
    "objectID": "Week9.html#reflection",
    "href": "Week9.html#reflection",
    "title": "8  Data processing",
    "section": "8.3 Reflection",
    "text": "8.3 Reflection\nExploring the concept of UHI, I’ve realized the complex interplay between natural and anthropogenic factors in influencing urban temperature patterns. While human activities contribute to the formation of heat islands, ecological settings, such as vegetation cover and geographical location, also play significant roles. This underscores the importance of considering both natural and human-induced factors in UHI research and mitigation strategies.\nReflecting on the articles reviewed, I’ve gained a deeper understanding of the multifaceted nature of the UHI phenomenon and its implications for urban sustainability and public health. Moving forward, I’m intrigued to explore interdisciplinary approaches that combine remote sensing, climatology, and urban planning to develop effective strategies for mitigating the impacts of heat islands and promoting resilient cities."
  },
  {
    "objectID": "Week9.html#references",
    "href": "Week9.html#references",
    "title": "8  Data processing",
    "section": "8.4 References",
    "text": "8.4 References\n\n\n\n\nArellano, B., and J. Roca. 2021. “REMOTE SENSING AND NIGHT TIME URBAN HEAT ISLAND.” The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences. https://www.proquest.com/conference-papers-proceedings/remote-sensing-night-time-urban-heat-island/docview/2585315470/se-2.\n\n\nBuo, Ioannis, Valerio Sagris, and Jaak Jaagus. 2023. “Generating Synthetic Daily Remote Sensing Products Suitable for Surface Heat Island and Heatwaves Assessments at Urban Scale.” International Journal of Environmental Science & Technology 20: 8599–8614. https://doi.org/10.1007/s13762-022-04510-3.\n\n\nChandrasekhar, Subrahmanyan. 2013. Radiative Transfer. Courier Corporation.\n\n\nImhoff, Marc L., Ping Zhang, Robert E. Wolfe, and Lahouari Bounoua. 2010. “Remote Sensing of the Urban Heat Island Effect Across Biomes in the Continental USA.” Remote Sensing of Environment 114 (3): 504–13. https://doi.org/https://doi.org/10.1016/j.rse.2009.10.008.\n\n\nLi, Zhao-Liang, Hua Wu, Si-Bo Duan, Wei Zhao, Huazhong Ren, Xiangyang Liu, Pei Leng, et al. 2023. “Satellite Remote Sensing of Global Land Surface Temperature: Definition, Methods, Products, and Applications.” Reviews of Geophysics 61 (1).\n\n\nP. Dash, F.-S. Olesen, F.-M. Göttsche, and H. Fischer. 2002. “Land Surface Temperature and Emissivity Estimation from Passive Sensor Data: Theory and Practice-Current Trends.” International Journal of Remote Sensing 23 (13): 2563–94. https://doi.org/10.1080/01431160110115041.\n\n\nPark, Yujin, Jean-Michel Guldmann, and Desheng Liu. 2021. “Impacts of Tree and Building Shades on the Urban Heat Island: Combining Remote Sensing, 3D Digital City and Spatial Regression Approaches.” Computers, Environment and Urban Systems 88: 101655. https://doi.org/https://doi.org/10.1016/j.compenvurbsys.2021.101655.\n\n\nPaschalis, A, TC Chakraborty, S Fatichi, N Meili, and G Manoli. 2021-06. “Urban Forests as Main Regulator of the Evaporative Cooling Effect in Cities,” 2021-06.\n\n\nPrata, AJ, V Caselles, C Coll, JA Sobrino, and C Ottle. 1995. “Thermal Remote Sensing of Land Surface Temperature from Satellites: Current Status and Future Prospects.” Remote Sensing Reviews 12 (3-4): 175–224.\n\n\nWang, Weimin, Kai Liu, Rong Tang, and Shudong Wang. 2019. “Remote Sensing Image-Based Analysis of the Urban Heat Island Effect in Shenzhen, China.” Physics and Chemistry of the Earth, Parts A/B/C 110: 168–75. https://doi.org/https://doi.org/10.1016/j.pce.2019.01.002.\n\n\nZhang, Z, A Paschalis, A Mijic, N Meili, G Manoli, M van Reeuwijk, and S Fatichi. 2022-07-01. “A Mechanistic Assessment of Urban Heat Island Intensities and Drivers Across Climates,” 2022-07-01."
  }
]